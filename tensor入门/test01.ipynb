{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.1、直接初始化\n",
    "    tensor可以直接从数据中创建。\n",
    "    数据类型是自动推断出来的。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2],\n        [3, 4]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.2、可以从Numpy中创建tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2],\n        [3, 4]], dtype=torch.int32)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.3、从其他tensor中初始化\n",
    "    新的tensor保留了参数tensor的属性（形状、数据类型），除非明确重写。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.0652, 0.7916],\n",
      "        [0.5088, 0.1966]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.4、使用随机数和常数初始化\n",
    "    shape是一个元组。在下面的代码中，它决定了输出tensor的维度。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.3058, 0.2309, 0.0910],\n",
      "        [0.1161, 0.5874, 0.9353]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.1、tensor的性质\n",
    "    tensor属性描述了它们的\n",
    "        形状；\n",
    "        数据类型；\n",
    "        存储它们的设备。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3、tensor的操作\n",
    "    100多种tensor操作，包括算术、线性代数、矩阵操作（转置、索引、切片）、采样等。\n",
    "    这些操作中的每一个都可以在GPU上运行（速度通常比在CPU上高）。\n",
    "    默认情况下，tensor是在CPU上创建的。我们需要使用.to方法明确地将tensor移动到GPU上（在检查GPU的可用性之后）。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 如果有的话，我们把我们的tensor移到GPU上\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.1、标准的类似numpy的索引和切分"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一行:  tensor([1., 1., 1., 1.])\n",
      "第一列： tensor([1., 1., 1., 1.])\n",
      "最后一列： tensor([1., 1., 1., 1.])\n",
      "让某一列全为0：\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4) # 二维的\n",
    "\n",
    "'''\n",
    "data = [[1, 2,3,4],\n",
    "        [5, 6,7,8],\n",
    "        [9,10,11,12],\n",
    "        [13,14,15,16],]\n",
    "tensor = torch.tensor(data)\n",
    "'''\n",
    "print('第一行: ', tensor[0])\n",
    "print('第一列：', tensor[:, 0])\n",
    "print('最后一列：', tensor[..., -1])\n",
    "print('让某一列全为0：')\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.2、连接tensor\n",
    "    可以使用torch.cat沿着给定的维度连接多个tensor。\n",
    "    也请看torch.stack，它是另一个与torch.cat有细微差别的tensor连接操作。\n",
    "    cat全称是concatnate,其中con是前缀，所以取cat代指concatenate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "t0 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.3、算术运算\n",
    "    1 矩阵乘法,对应的行列元素相乘再相加\n",
    "        A=a11 a12 a13   B=b11 b12   a11b11+a12b21+a13b31  a11b12+a12b22+a13b32\n",
    "          a21 a22 a23     b21 b22   a21b11+a22b21+a23b31  a21b12+a22b22+a23b32\n",
    "                          b31 b32\n",
    "    2 矩阵元素相乘，对应位置的元素相乘\n",
    "        如：A= a11 a12 a13   B=b11 b12 b13   a11b11 a12b12 a13b13\n",
    "              a21 a22 a23     b21 b22 b23   a21b21 a22b22 a23b23"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 4., 4., 4.],\n        [4., 4., 4., 4.],\n        [4., 4., 4., 4.],\n        [4., 4., 4., 4.]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y1 y2 y3 的值相同，都是矩阵乘法的结果\n",
    "tensor = torch.ones(4, 4)\n",
    "y1 = tensor @ tensor\n",
    "\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor,tensor.T,out=y3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) \n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]]) \n",
      " tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵元素相乘\n",
    "data = [[1, 2,3,4],\n",
    "        [5, 6,7,8],\n",
    "        [9,10,11,12],\n",
    "        [13,14,15,16],]\n",
    "tensor2 = torch.tensor(data)\n",
    "z1 = tensor*tensor2\n",
    "\n",
    "z2 = tensor.mul(tensor2)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor,tensor2,out=z3)\n",
    "\n",
    "print(tensor,'\\n',tensor2,'\\n',z3)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
